{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Predict weather using pyspark\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Predict weather using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "# Spark Context\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Spark Context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 21:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "master = \"local[*]\"\n",
    "\n",
    "app_name = \"Predict Weather Using Spark\"\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Prep"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train = spark.read.format('csv')\\\n",
    "                    .option('header', True).option('escape', '\"').option('inferSchema', True)\\\n",
    "                    .load('data/weather-training-data.csv')\n",
    "\n",
    "df_test = spark.read.format('csv')\\\n",
    "                    .option('header', True).option('escape', '\"').option('inferSchema', True)\\\n",
    "                    .load('data/weather-test-data.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data 99516, Test data 42677\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data {df_train.count()}, Test data {df_test.count()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data   summary    row ID  Location             MinTemp             MaxTemp  \\\n",
      "0   count     99516     99516               99073               99286   \n",
      "1    mean      None      None  12.176265985687314  23.218513184134757   \n",
      "2  stddev      None      None   6.390882290565232   7.115072398372783   \n",
      "3     min      Row0  Adelaide                -8.5                -4.1   \n",
      "4     max  Row99999   Woomera                33.9                48.1   \n",
      "\n",
      "             Rainfall        Evaporation           Sunshine WindGustDir  \\\n",
      "0               98537              56985              52199       99516   \n",
      "1  2.3530237372762692   5.46131964552076  7.615090327400976        None   \n",
      "2   8.487865726637184  4.162490444622897  3.783007646224733        None   \n",
      "3                 0.0                0.0                0.0           E   \n",
      "4               371.0               86.2               14.5         WSW   \n",
      "\n",
      "       WindGustSpeed  ...         Humidity9am        Humidity3pm  \\\n",
      "0              93036  ...               98283              97010   \n",
      "1  39.97696590567092  ...   68.86637567025834  51.43329553654262   \n",
      "2  13.58152350853846  ...  19.074951346357153  20.77761619524104   \n",
      "3                  6  ...                   0                  0   \n",
      "4                135  ...                 100                100   \n",
      "\n",
      "          Pressure9am         Pressure3pm            Cloud9am  \\\n",
      "0               89768               89780               61944   \n",
      "1  1017.6846381784097  1015.2862042214292   4.447985277024409   \n",
      "2   7.110166247607829    7.04518877022016  2.8865800508918316   \n",
      "3               980.5               978.2                   0   \n",
      "4              1041.0              1039.6                   9   \n",
      "\n",
      "            Cloud3pm             Temp9am             Temp3pm RainToday  \\\n",
      "0              59514               98902               97612     99516   \n",
      "1  4.519121551231643  16.970041050737137   21.68134040896606      None   \n",
      "2  2.716618014549538   6.488960793858915  6.9316814716127775      None   \n",
      "3                  0                -7.0                -5.1        NA   \n",
      "4                  9                40.2                46.7       Yes   \n",
      "\n",
      "          RainTomorrow  \n",
      "0                99516  \n",
      "1  0.22467743880381044  \n",
      "2    0.417371821953755  \n",
      "3                    0  \n",
      "4                    1  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data {df_train.describe().toPandas().head()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data   summary   row ID  Location             MinTemp             MaxTemp  \\\n",
      "0   count    42677     42677               42483               42585   \n",
      "1    mean     None      None  12.210032248193444  23.246067864271506   \n",
      "2  stddev     None      None  6.4321216400080825   7.123596236045857   \n",
      "3     min     Row0  Adelaide                -8.2                -4.8   \n",
      "4     max  Row9999   Woomera                31.8                47.0   \n",
      "\n",
      "             Rainfall         Evaporation            Sunshine WindGustDir  \\\n",
      "0               42250               24365               22178       42677   \n",
      "1  2.3428615384614444  5.4897147547711915  7.6478311840563205        None   \n",
      "2   8.412105837942626   4.248849509250146  3.7780187468829842        None   \n",
      "3                 0.0                 0.0                 0.0           E   \n",
      "4               278.4               145.0                14.3         WSW   \n",
      "\n",
      "        WindGustSpeed  ...        WindSpeed3pm         Humidity9am  \\\n",
      "0               39887  ...               41882               42136   \n",
      "1  40.001378895379446  ...  18.607516355474907   68.79117619138029   \n",
      "2  13.605914806418527  ...   8.806915848592858  18.996115195779723   \n",
      "3                   7  ...                   0                   1   \n",
      "4                 122  ...                  83                 100   \n",
      "\n",
      "          Humidity3pm         Pressure9am         Pressure3pm  \\\n",
      "0               41573               38411               38432   \n",
      "1   51.59767156567965  1017.5815912108507  1015.1927918661462   \n",
      "2  20.844525483441345   7.094069990680769    7.01640800324464   \n",
      "3                   1               982.2               977.1   \n",
      "4                 100              1040.4              1038.9   \n",
      "\n",
      "             Cloud9am            Cloud3pm            Temp9am  \\\n",
      "0               26592               25585              42387   \n",
      "1  4.4120412154031285  4.4660543287082275   17.0282657418548   \n",
      "2  2.8879274609743675  2.7296404237065905  6.501770925604385   \n",
      "3                   0                   0               -7.2   \n",
      "4                   8                   8               39.4   \n",
      "\n",
      "             Temp3pm RainToday  \n",
      "0              41855     42677  \n",
      "1  21.70098196153381      None  \n",
      "2  6.951426386081021      None  \n",
      "3               -5.4        NA  \n",
      "4               45.4       Yes  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Test data {df_test.describe().toPandas().head()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check Missing Data / Null Value\n",
    "Check Missing Value and Drop Null/NaN values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================>                                      (1 + 2) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|row ID|Location|MinTemp|MaxTemp|Rainfall|Evaporation|Sunshine|WindGustDir|WindGustSpeed|WindDir9am|WindDir3pm|WindSpeed9am|WindSpeed3pm|Humidity9am|Humidity3pm|Pressure9am|Pressure3pm|Cloud9am|Cloud3pm|Temp9am|Temp3pm|RainToday|RainTomorrow|\n",
      "+------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "|     0|       0|    443|    230|     979|      42531|   47317|          0|         6480|         0|         0|         935|        1835|       1233|       2506|       9748|       9736|   37572|   40002|    614|   1904|        0|           0|\n",
      "+------+--------+-------+-------+--------+-----------+--------+-----------+-------------+----------+----------+------------+------------+-----------+-----------+-----------+-----------+--------+--------+-------+-------+---------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df_train.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_train.columns]).show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Identify list of columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['row ID', 'Location', 'MinTemp', 'MaxTemp', 'Rainfall', 'Evaporation', 'Sunshine', 'WindGustDir', 'WindGustSpeed', 'WindDir9am', 'WindDir3pm', 'WindSpeed9am', 'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am', 'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm', 'RainToday', 'RainTomorrow']\n"
     ]
    }
   ],
   "source": [
    "column_df_train = df_train.columns\n",
    "print(column_df_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Engineering\n",
    "Select a list of features to put into ML Model. Use K Means Clustering. <br>\n",
    "1. implement StringIndexer to index columns \"Location\", \"WindGustDir\", \"WindDir*\" and \"RainToday\"\n",
    "2. Implement OHE\n",
    "3. Implement VA\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Prep in Feature Engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Location_Index_OHE does not exist. Available: Location, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, Temp3pm, RainToday, RainTomorrow",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[12], line 22\u001B[0m\n\u001B[1;32m     20\u001B[0m input_cols_VA \u001B[38;5;241m=\u001B[39m [col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m output_cols_OHE \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m     21\u001B[0m assembler \u001B[38;5;241m=\u001B[39m VectorAssembler(inputCols\u001B[38;5;241m=\u001B[39minput_cols_VA, outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrawFeatures\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 22\u001B[0m df_output \u001B[38;5;241m=\u001B[39m \u001B[43massembler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/envs/DataEngineeringPlayground/lib/python3.10/site-packages/pyspark/ml/base.py:262\u001B[0m, in \u001B[0;36mTransformer.transform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy(params)\u001B[38;5;241m.\u001B[39m_transform(dataset)\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 262\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParams must be a param map but got \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mtype\u001B[39m(params))\n",
      "File \u001B[0;32m~/miniforge3/envs/DataEngineeringPlayground/lib/python3.10/site-packages/pyspark/ml/wrapper.py:400\u001B[0m, in \u001B[0;36mJavaTransformer._transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    397\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transfer_params_to_java()\n\u001B[0;32m--> 400\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_java_obj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m)\u001B[49m, dataset\u001B[38;5;241m.\u001B[39msparkSession)\n",
      "File \u001B[0;32m~/miniforge3/envs/DataEngineeringPlayground/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[0;32m~/miniforge3/envs/DataEngineeringPlayground/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    192\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    194\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    195\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 196\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    198\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mIllegalArgumentException\u001B[0m: Location_Index_OHE does not exist. Available: Location, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, Temp3pm, RainToday, RainTomorrow"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier, LogisticRegression\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "\n",
    "\"\"\"Pass all features to put into the ML Model for now\"\"\"\n",
    "\n",
    "df_train = df_train.drop(\"row ID\").drop(\"prediction\")\n",
    "input_cols = [col for col in df_train.columns if col not in [\"RainTomorrow\"]]\n",
    "output_cols = [f\"{col}_Index\" for col in input_cols]\n",
    "\n",
    "# String Indexer\n",
    "string_indexer = StringIndexer(inputCols=input_cols, outputCols=output_cols).setHandleInvalid(\"keep\")\n",
    "indexed = string_indexer.fit(df_train).transform(df_train)\n",
    "\n",
    "# One Hot Encoder\n",
    "output_cols_OHE = [f\"{col}_OHE\" for col in output_cols if col not in [\"RainTomorrow\"]]\n",
    "encoder = OneHotEncoder(inputCols=output_cols, outputCols=output_cols_OHE)\n",
    "encoded = encoder.fit(indexed).transform(indexed)\n",
    "\n",
    "# VectorAssembler\n",
    "input_cols_VA = [col for col in output_cols_OHE if col != 'label']\n",
    "assembler = VectorAssembler(inputCols=input_cols_VA, outputCol='rawFeatures')\n",
    "df_output = assembler.transform(df_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Predictions\n",
    "Use the above result to make predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipelineModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m predictions \u001B[38;5;241m=\u001B[39m \u001B[43mpipelineModel\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(df_train)\u001B[38;5;241m.\u001B[39mwithColumnRenamed(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscaledFeatures\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfeatures\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      2\u001B[0m predictions\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\u001B[38;5;241m.\u001B[39morderBy(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocation\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprediction\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mshow(\u001B[38;5;241m5\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'pipelineModel' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = pipelineModel.transform(df_train).withColumnRenamed('scaledFeatures', 'features')\n",
    "predictions.groupBy('label','location', 'prediction').count().orderBy('location', 'prediction').show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating Model\n",
    "Evaluate the model by calculating the **Silhouette Score**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette_score = evaluator.evaluate(predictions)\n",
    "print(\"The Silhouette score with squared euclidean distance of GBT Classification Clustering is : \" + str(silhouette_score))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Training Summary:** AUC, accuracy, recall and precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions.printSchema()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gbt_prediction_and_labels = predictions.select('label', 'prediction').rdd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "gbt_evaluator = MulticlassMetrics(gbt_prediction_and_labels)\n",
    "print(\"Training summary regarding the Gradient Boosted Tree Classifier\")\n",
    "print(f\"Accuracy: {gbt_evaluator.accuracy}\")\n",
    "\n",
    "gbt_labels = (predictions.select(\"label\").distinct().collect()[1])\n",
    "\n",
    "for i in sorted(gbt_labels):\n",
    "    print(\"Label %s precision = %s\" % (i, gbt_evaluator.precision(i)))\n",
    "    print(\"Label %s recall = %s\" % (i, gbt_evaluator.recall(i)))\n",
    "    print(\"Label %s F1 Measure = %s\" % (i, gbt_evaluator.fMeasure(i, beta=1.0)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Binary Classification Evaluator**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "gbt_auc_evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "gbt_auc = gbt_auc_evaluator.evaluate(predictions)\n",
    "print(f\"AUC: {gbt_auc}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Feature Analysis\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the top 5 most import features"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions.select(\"RainTomorrow\", \"features\", \"prediction\")\\\n",
    "    .groupBy(\"RainTomorrow\", \"features\", \"prediction\").count().orderBy(\"count\", ascending=False).show(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## ROC Curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Use this UDF to cast probability to array\n",
    "cast_to_array = F.udf(lambda i: i.toArray().tolist(), T.ArrayType(T.FloatType()))\n",
    "\n",
    "gbt_roc_df = predictions.withColumn(\"probability\", cast_to_array(\"probability\"))\n",
    "gbt_roc_prob_df = gbt_roc_df.select(gbt_roc_df.probability[0].alias(\"negativeProb\"), gbt_roc_df.probability[1].alias(\"positiveProb\"), \"label\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Method to build Confusion matrix\n",
    "def build_confusion_matrix(prediction):\n",
    "    # Calculate the elements of the confusion matrix\n",
    "    TN = prediction.filter(\"prediction = 0 AND label = 0\").count()\n",
    "    TP = prediction.filter(\"prediction = 1 AND label = 1\").count()\n",
    "    FN = prediction.filter(\"prediction = 0 AND label = 1\").count()\n",
    "    FP = prediction.filter(\"prediction = 1 AND label = 0\").count()\n",
    "    return TP,TN,FP,FN"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compute **confusion matrix**, set TP threshold at 0.8"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "threshold = 0.8\n",
    "gbt_roc_prob_df.withColumn(\"prediction\",F.when(gbt_roc_prob_df.positiveProb > threshold,1).otherwise(0))\n",
    "gbt_tp,gbt_tn,gbt_fp,gbt_fn = build_confusion_matrix(gbt_roc_prob_df)\n",
    "gbt_tpr = gbt_tp/(gbt_tp + gbt_fn)\n",
    "gbt_fpr = gbt_fp/(gbt_fp + gbt_tn)\n",
    "print(\"TPR:\",gbt_tpr,\"FPR: \",gbt_fpr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot ROC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(gbt_fpr,gbt_tpr)\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
